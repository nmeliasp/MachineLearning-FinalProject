{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import numpy\n",
    "import pandas\n",
    "import math\n",
    "#sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from tester import main\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of people in data set =  146\n",
      "Number of features for every person in the data set:\n",
      "21\n",
      "There are 18 POIs\n",
      "There are 128 non POIs\n"
     ]
    }
   ],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi','salary','bonus','total_stock_value','exercised_stock_options','total_payments','long_term_incentive'] # You will need to use more features\n",
    "financial_features_list = ['poi','salary','bonus','deferral_payments','deferred_income','director_fees','exercised_stock_options','expenses','loan_advances','long_term_incentive','restricted_stock','restricted_stock_deferred','total_payments','total_stock_value']\n",
    "email_feature_list = ['poi','to_messages', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi']\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "print 'number of people in data set = ', len(data_dict)\n",
    "print 'Number of features for every person in the data set:\\n', len(data_dict['LAY KENNETH L'].keys())\n",
    "# of POIs\n",
    "count = 0\n",
    "for p in data_dict:\n",
    "    for i in data_dict[p]:\n",
    "        if i == \"poi\":\n",
    "            if data_dict[p][i] == 1:\n",
    "                count +=1\n",
    "print (\"There are %d POIs\" % (count))\n",
    "print (\"There are %d non POIs\" %(146-count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data set before removing outlier =  145\n",
      "length of data set after removing outlier =  145\n"
     ]
    }
   ],
   "source": [
    "### Task 2: Remove outliers\n",
    "''' as seen in the outliers section, we saw that there was a \"Total\" entry in the data set that was a clear outlier.\n",
    "thus we are removing it\n",
    "'''\n",
    "print \"length of data set before removing outlier = \", len(data_dict)\n",
    "data_dict.pop(\"TOTAL\",0)\n",
    "print \"length of data set after removing outlier = \", len(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "# Helper function\n",
    "def computeFraction( poi_messages, all_messages ):\n",
    "    \"\"\" given a number messages to/from POI (numerator) \n",
    "        and number of all messages to/from a person (denominator),\n",
    "        return the fraction of messages to/from that person\n",
    "        that are from/to a POI\n",
    "   \"\"\"\n",
    "    fraction = 0.\n",
    "    poi_messages = float(poi_messages)\n",
    "    all_messages = float(all_messages)\n",
    "    \n",
    "    #print \"poi_messages = \", poi_messages\n",
    "    #print \"all_messages = \", all_messages\n",
    "    #if poi_messages==\"NaN\" or all_messages ==\"NaN\" or all_messages== 0 :\n",
    "    #    fraction = 0.0\n",
    "    if math.isnan(poi_messages) or math.isnan(all_messages):\n",
    "        fraction = 0.0\n",
    "    else: \n",
    "        fraction = float(poi_messages/all_messages)\n",
    "        if math.isnan(fraction):\n",
    "            fraction = 0.0\n",
    "    return fraction\n",
    "# Computer fraction from poi and to poi\n",
    "for name in data_dict:\n",
    "    data_dict[name]['fraction_from_poi'] = computeFraction( data_dict[name]['from_poi_to_this_person'],\\\n",
    "                                                           data_dict[name]['to_messages'] )\n",
    "    data_dict[name]['fraction_to_poi'] = computeFraction( data_dict[name]['from_this_person_to_poi'],\\\n",
    "                                                           data_dict[name]['from_messages'] )\n",
    "    \n",
    "\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset=data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bonus</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>email_address</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>fraction_from_poi</th>\n",
       "      <th>fraction_to_poi</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>...</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "      <th>poi</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>salary</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>METTS MARK</th>\n",
       "      <td>600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mark.metts@enron.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94299</td>\n",
       "      <td>0.047088</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1740</td>\n",
       "      <td>False</td>\n",
       "      <td>585062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365788</td>\n",
       "      <td>702</td>\n",
       "      <td>807</td>\n",
       "      <td>1061827</td>\n",
       "      <td>585062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAXTER JOHN C</th>\n",
       "      <td>1200000</td>\n",
       "      <td>1295738</td>\n",
       "      <td>-1386055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6680544</td>\n",
       "      <td>11200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1586055</td>\n",
       "      <td>2660303</td>\n",
       "      <td>False</td>\n",
       "      <td>3942714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>267102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5634343</td>\n",
       "      <td>10623258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELLIOTT STEVEN</th>\n",
       "      <td>350000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-400729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>steven.elliott@enron.com</td>\n",
       "      <td>4890344</td>\n",
       "      <td>78552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12961</td>\n",
       "      <td>False</td>\n",
       "      <td>1788391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>170941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211725</td>\n",
       "      <td>6678735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CORDES WILLIAM R</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bill.cordes@enron.com</td>\n",
       "      <td>651850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>386335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1038185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HANNON KEVIN P</th>\n",
       "      <td>1500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3117011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kevin.hannon@enron.com</td>\n",
       "      <td>5538001</td>\n",
       "      <td>34039</td>\n",
       "      <td>0.030622</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>1617011</td>\n",
       "      <td>11350</td>\n",
       "      <td>True</td>\n",
       "      <td>853064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>243293</td>\n",
       "      <td>1035</td>\n",
       "      <td>1045</td>\n",
       "      <td>288682</td>\n",
       "      <td>6391065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    bonus deferral_payments deferred_income director_fees  \\\n",
       "METTS MARK         600000               NaN             NaN           NaN   \n",
       "BAXTER JOHN C     1200000           1295738        -1386055           NaN   \n",
       "ELLIOTT STEVEN     350000               NaN         -400729           NaN   \n",
       "CORDES WILLIAM R      NaN               NaN             NaN           NaN   \n",
       "HANNON KEVIN P    1500000               NaN        -3117011           NaN   \n",
       "\n",
       "                             email_address exercised_stock_options expenses  \\\n",
       "METTS MARK            mark.metts@enron.com                     NaN    94299   \n",
       "BAXTER JOHN C                          NaN                 6680544    11200   \n",
       "ELLIOTT STEVEN    steven.elliott@enron.com                 4890344    78552   \n",
       "CORDES WILLIAM R     bill.cordes@enron.com                  651850      NaN   \n",
       "HANNON KEVIN P      kevin.hannon@enron.com                 5538001    34039   \n",
       "\n",
       "                  fraction_from_poi  fraction_to_poi from_messages  \\\n",
       "METTS MARK                 0.047088         0.034483            29   \n",
       "BAXTER JOHN C              0.000000         0.000000           NaN   \n",
       "ELLIOTT STEVEN             0.000000         0.000000           NaN   \n",
       "CORDES WILLIAM R           0.013089         0.000000            12   \n",
       "HANNON KEVIN P             0.030622         0.656250            32   \n",
       "\n",
       "                        ...        long_term_incentive    other    poi  \\\n",
       "METTS MARK              ...                        NaN     1740  False   \n",
       "BAXTER JOHN C           ...                    1586055  2660303  False   \n",
       "ELLIOTT STEVEN          ...                        NaN    12961  False   \n",
       "CORDES WILLIAM R        ...                        NaN      NaN  False   \n",
       "HANNON KEVIN P          ...                    1617011    11350   True   \n",
       "\n",
       "                 restricted_stock restricted_stock_deferred  salary  \\\n",
       "METTS MARK                 585062                       NaN  365788   \n",
       "BAXTER JOHN C             3942714                       NaN  267102   \n",
       "ELLIOTT STEVEN            1788391                       NaN  170941   \n",
       "CORDES WILLIAM R           386335                       NaN     NaN   \n",
       "HANNON KEVIN P             853064                       NaN  243293   \n",
       "\n",
       "                 shared_receipt_with_poi to_messages total_payments  \\\n",
       "METTS MARK                           702         807        1061827   \n",
       "BAXTER JOHN C                        NaN         NaN        5634343   \n",
       "ELLIOTT STEVEN                       NaN         NaN         211725   \n",
       "CORDES WILLIAM R                      58         764            NaN   \n",
       "HANNON KEVIN P                      1035        1045         288682   \n",
       "\n",
       "                 total_stock_value  \n",
       "METTS MARK                  585062  \n",
       "BAXTER JOHN C             10623258  \n",
       "ELLIOTT STEVEN             6678735  \n",
       "CORDES WILLIAM R           1038185  \n",
       "HANNON KEVIN P             6391065  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify new features have been added correctly to the data set\n",
    "df = pandas.DataFrame.from_records(list(data_dict.values()))\n",
    "employees = pandas.Series(list(data_dict.keys()))\n",
    "\n",
    "# set the index of df to be the employees series:\n",
    "df.set_index(employees, inplace=True)\n",
    "                          \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEFCAYAAADqujDUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1314d160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize new features\n",
    "x = df['fraction_from_poi'].values\n",
    "y = df['fraction_to_poi'].values\n",
    "c = df['poi'].tolist()\n",
    "plt.scatter(x,y,c=c,cmap='cool')\n",
    "plt.xlabel(\"fraction_from_poi\")\n",
    "plt.ylabel(\"fraction_to_poi\")\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, i am going to extract the features i defined in my list above and select the top 4 features from the financial features and from the email features to help aid my classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing financial_features k scores: \n",
      "[ 18.57570327  21.06000171   0.21705893  11.59554766   2.10765594\n",
      "  25.09754153   6.23420114   7.2427304   10.07245453   9.34670079\n",
      "   0.06498431   8.86672154  24.46765405]\n",
      "printing email_features k scores: \n",
      "[ 0.29296869  2.43137891  0.46640016  1.0853069   4.61945732]\n"
     ]
    }
   ],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "\n",
    "# extract the 4 strongest features from the financial data with SelectKBest\n",
    "financial_data=featureFormat(my_dataset, financial_features_list, sort_keys=True)\n",
    "financial_labels, financial_features = targetFeatureSplit(financial_data)\n",
    "\n",
    "# select best features using SelectKBest\n",
    "test1=SelectKBest(k='all')\n",
    "fit1=test1.fit(financial_features,financial_labels)\n",
    "\n",
    "#numpy.set_printoptions(precision=10)\n",
    "print \"printing financial_features k scores: \\n\", (fit1.scores_)\n",
    "\n",
    "feature=fit1.transform(financial_features)\n",
    "# summarize selected features\n",
    "#print(feature)\n",
    "\n",
    "#print features\n",
    "#print labels\n",
    "#print financial_features[3]\n",
    "\n",
    "#extract the 4 strongest features from email data with SelectKBest\n",
    "email_data=featureFormat(my_dataset, email_feature_list, sort_keys = True)\n",
    "email_labels, email_features=targetFeatureSplit(email_data)\n",
    "\n",
    "test2=SelectKBest(k='all')\n",
    "fit2=test2.fit(email_features,email_labels)\n",
    "\n",
    "print \"printing email_features k scores: \\n\", (fit2.scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running SelectKBest seperately on the Finanical Data and Email Data. The following features were identified as the top 4 from each group:\n",
    "* Financial Data:\n",
    "    - 'exercised_stock_options'\t25.09754153\n",
    "    - 'total_stock_value'\t24.46765405\n",
    "    - 'bonus'\t21.06000171\n",
    "    - 'salary'\t18.57570327\n",
    "\n",
    "\n",
    "\n",
    "* Email Data:\n",
    "    - 'shared_receipt_with_poi'\t4.61945732\n",
    "    - 'from_poi_to_this_person'\t2.43137891\n",
    "    - 'from_this_person_to_poi'\t1.0853069\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now i am going to combine these feature to extract the top overal features from this set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing financial_email_features k scores: \n",
      "[ 21.71552656  21.05899501  17.8573624   15.14904119   6.8822438\n",
      "   4.1460684    1.90840396   0.24111688]\n"
     ]
    }
   ],
   "source": [
    "financial_email_features=['poi','exercised_stock_options','total_stock_value','bonus','salary','shared_receipt_with_poi','from_poi_to_this_person','from_this_person_to_poi','from_messages']\n",
    "financial_email_data=featureFormat(my_dataset, financial_email_features, sort_keys = True)\n",
    "labels, financial_email_features=targetFeatureSplit(financial_email_data)\n",
    "\n",
    "# select 4 best features using SelectKBest\n",
    "test3=SelectKBest(k='all')\n",
    "fit3=test3.fit(financial_email_features,labels)\n",
    "\n",
    "#numpy.set_printoptions(precision=10)\n",
    "print \"printing financial_email_features k scores: \\n\", (fit3.scores_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running SelectKBest on the top 4 features from each group combined, I observed the following K scores:\n",
    "\n",
    "Feature \t               Score\n",
    "- 'exercised_stock_options'\t21.71552656\n",
    "- 'total_stock_value'\t21.05899501\n",
    "- 'bonus'\t17.8573624\n",
    "- 'salary'\t15.14904119\n",
    "- 'shared_receipt_with_poi'\t6.8822438\n",
    "- 'from_poi_to_this_person'\t4.1460684\n",
    "- 'from_this_person_to_poi'\t1.90840396\n",
    "- 'from_messages'\t0.24111688\n",
    "\n",
    "\n",
    " Features from the financial data seem to be the best features to use to input them into our classifier. I'm going to compare this when I run SelectKBest on all features and also to SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing full_features k scores: \n",
      "[ 18.57570327   0.21705893   8.86672154   7.2427304   21.06000171\n",
      "   0.06498431  11.59554766  24.46765405   6.23420114  25.09754153\n",
      "   4.20497086  10.07245453   9.34670079   2.10765594   1.69882435\n",
      "   5.34494152   0.1641645    2.42650813   8.74648553   3.21076192\n",
      "  16.64170707]\n"
     ]
    }
   ],
   "source": [
    "full_feature_list=['poi','salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees','to_messages','from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi','fraction_from_poi','fraction_to_poi']\n",
    "full_data=featureFormat(my_dataset, full_feature_list, sort_keys = True)\n",
    "labels, full_features=targetFeatureSplit(full_data)\n",
    "\n",
    "# select best features using SelectKBest\n",
    "test4=SelectKBest(k='all')\n",
    "fit4=test4.fit(full_features,labels)\n",
    "\n",
    "#numpy.set_printoptions(precision=10)\n",
    "print \"printing full_features k scores: \\n\", (fit4.scores_)\n",
    "\n",
    "feature=fit4.transform(full_features)\n",
    "\n",
    "#print full_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running SelectKBest on the entire feature list, excluding email address, I observed the following K scores:\n",
    "\n",
    "Feature \tScore\n",
    "- 'exercised_stock_options'\t25.09754153\n",
    "- 'total_stock_value'\t24.46765405\n",
    "- 'bonus'\t21.06000171\n",
    "- 'salary'\t18.57570327\n",
    "- 'deferred_income'\t11.59554766\n",
    "- 'long_term_incentive'\t10.07245453\n",
    "- 'restricted_stock'\t9.34670079\n",
    "- total_payments'\t8.86672154\n",
    "- 'shared_receipt_with_poi'\t8.74648553\n",
    "- 'loan_advances'\t7.2427304\n",
    "- 'expenses'\t6.23420114\n",
    "- 'from_poi_to_this_person'\t5.34494152\n",
    "- 'other'\t4.20497086\n",
    "- 'from_this_person_to_poi'\t2.42650813\n",
    "- 'director_fees'\t2.10765594\n",
    "- 'to_messages'\t1.69882435\n",
    "- 'deferral_payments'\t0.21705893\n",
    "- 'from_messages'\t0.1641645\n",
    "- 'restricted_stock_deferred'\t0.06498431\n",
    "\n",
    "\n",
    "As seen, the top 4 features from this list is the same when I picked out the top 4 features from each group and ran SelectKBest on the combined list. Lets see if this is the same when using Selectpercentile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing full_features k scores: \n",
      "[ 18.57570327   0.21705893   8.86672154   7.2427304   21.06000171\n",
      "   0.06498431  11.59554766  24.46765405   6.23420114  25.09754153\n",
      "   4.20497086  10.07245453   9.34670079   2.10765594   1.69882435\n",
      "   5.34494152   0.1641645    2.42650813   8.74648553]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "test5=SelectPercentile(f_classif, percentile=10)\n",
    "fit5=test5.fit(full_features,labels)\n",
    "\n",
    "#numpy.set_printoptions(precision=10)\n",
    "print \"printing full_features k scores: \\n\", (fit5.scores_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running SelectPercentile, I get the same scores as I did with SelectKBest. I think i found the features i want to use with my classifier. \n",
    "\n",
    "Now that I identified the features i want to use,  I am going to try a variety of classifiers without any parameter tunes to explore accuracy, recall, precision, and F1 metrics to help me identify which classifiers I should focus on when I go to tune them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.897435897436\n",
      "precision =  0.0\n",
      "recall =  0.0\n",
      "\n",
      "Running Stratisfied Shuffle Split cross validation to compare recall\n",
      "\n",
      "printing mean of Stratisfied Shuffle Split\n",
      "mean =  0.25\n",
      "\n",
      "\n",
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.84677\tPrecision: 0.50312\tRecall: 0.32300\tF1: 0.39342\tF2: 0.34791\n",
      "\tTotal predictions: 13000\tTrue positives:  646\tFalse positives:  638\tFalse negatives: 1354\tTrue negatives: 10362\n",
      "\n",
      "Accuracy is  0.897435897436\n",
      "precision =  0.5\n",
      "recall =  0.25\n",
      "\n",
      "Running Stratisfied Shuffle Split cross validation to compare recall\n",
      "\n",
      "printing mean of Stratisfied Shuffle Split\n",
      "mean =  0.2\n",
      "\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "\tAccuracy: 0.79015\tPrecision: 0.32347\tRecall: 0.33350\tF1: 0.32841\tF2: 0.33145\n",
      "\tTotal predictions: 13000\tTrue positives:  667\tFalse positives: 1395\tFalse negatives: 1333\tTrue negatives: 9605\n",
      "\n",
      "Accuracy is  0.897435897436\n",
      "precision =  0.5\n",
      "recall =  0.25\n",
      "\n",
      "Running Stratisfied Shuffle Split cross validation to compare recall\n",
      "\n",
      "printing mean of Stratisfied Shuffle Split\n",
      "mean =  0.15\n",
      "\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\tAccuracy: 0.84569\tPrecision: 0.49691\tRecall: 0.24150\tF1: 0.32503\tF2: 0.26917\n",
      "\tTotal predictions: 13000\tTrue positives:  483\tFalse positives:  489\tFalse negatives: 1517\tTrue negatives: 10511\n",
      "\n",
      "Accuracy is  0.897435897436\n",
      "precision =  0.0\n",
      "recall =  0.0\n",
      "\n",
      "Running Stratisfied Shuffle Split cross validation to compare recall\n",
      "\n",
      "printing mean of Stratisfied Shuffle Split\n",
      "mean =  0.0\n",
      "\n",
      "\n",
      "Got a divide by zero when trying out: SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n",
      "Accuracy is  0.923076923077\n",
      "precision =  1.0\n",
      "recall =  0.25\n",
      "\n",
      "Running Stratisfied Shuffle Split cross validation to compare recall\n",
      "\n",
      "printing mean of Stratisfied Shuffle Split\n",
      "mean =  0.3\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "\tAccuracy: 0.86615\tPrecision: 0.65971\tRecall: 0.26850\tF1: 0.38166\tF2: 0.30463\n",
      "\tTotal predictions: 13000\tTrue positives:  537\tFalse positives:  277\tFalse negatives: 1463\tTrue negatives: 10723\n",
      "\n",
      "printing summary of recall score from Classifiers, \n",
      "[0.0, 0.25, 0.25, 0.0, 0.25]\n",
      "printing summary of mean recall scores from Stratisfied Shuffle Split CV, \n",
      "[0.25, 0.25, 0.29999999999999999, 0.0, 0.29999999999999999]\n"
     ]
    }
   ],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "\n",
    "# use and extract features identified in previous cell for classifers. \n",
    "features_list=['poi','salary',  'bonus', 'total_stock_value', 'exercised_stock_options']\n",
    "data=featureFormat(my_dataset, features_list, sort_keys=True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# split data into training/testing data useing cross validation\n",
    "features_train, features_test, labels_train, labels_test=\\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "# list of classifiers and lists to track metric scores\n",
    "clf_list=[GaussianNB(),tree.DecisionTreeClassifier(),RandomForestClassifier(),SVC(),KNeighborsClassifier()]\n",
    "recall_list=[]\n",
    "mean_recall_list=[]\n",
    "\n",
    "# Using Stratisfied ShuffleSplit since there is an imbalance of POIs and non P\n",
    "cv = StratifiedShuffleSplit(random_state=42)\n",
    "# loop through each classifier and capture evaluation metrics\n",
    "for c in clf_list:\n",
    "    clf = c\n",
    "    clf.fit(features_train,labels_train)\n",
    "    pred=clf.predict(features_test)\n",
    "    #print pred\n",
    "    print \"Accuracy is \",clf.score(features_test, labels_test)\n",
    "    print \"precision = \", precision_score(labels_test,pred)\n",
    "    print \"recall = \", recall_score(labels_test,pred)\n",
    "    recall_list.append(recall_score(labels_test,pred))\n",
    "    print \"\\nRunning Stratisfied Shuffle Split cross validation to compare recall\\n\"\n",
    "    print \"printing mean of Stratisfied Shuffle Split\"\n",
    "    print \"mean = \",cross_val_score(clf,features,labels,cv=cv.split(features,labels),scoring='recall').mean()\n",
    "    mean_recall_list.append(cross_val_score(clf,features,labels,cv=cv.split(features,labels),scoring='recall').mean())\n",
    "    print \"\\n\"\n",
    "    dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "    main()\n",
    "print \"printing summary of recall score from Classifiers, \\n\", recall_list\n",
    "#print \"printing summary of accuracy scores from Classifiers, \\n\", accuracy_list\n",
    "print \"printing summary of mean recall scores from Stratisfied Shuffle Split CV, \\n\", mean_recall_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I didnâ€™t have a particular algorithm to try in mind, I chose to iterate through several classifiers to evaluate different metrics without making any parameter tunes to get a baseline of how each classifier performs. This will help me choose which classifier to focus tuning parameters on. When iterating, I captured the accuracy of a feature train/test split as well as a mean accuracy when running Stratisfied Shuffle split validation test to compare the two scores. I felt it was necessary to perform Stratisfied Shuffle split cross validation because of the imbalance in POIs and non POIs and to ensure all data is included in a test and a training procedure. Additionally, I dumped out the classifier, dataset, and feature list in each iteration so I can call tester.py. I recorded all the scores the tester file provides scores of each classifier. After iterating through each classier, I observed the following metrics:\n",
    "\n",
    "\n",
    "|Classifier|Accuracy|Precision|Recall|Mean 10 Fold CV Accuracy|F1 Score|\n",
    "|-------|------|------|-------|------|------|\n",
    "|Naive Bayes |.847|.503|.323|.855|.393|\n",
    "|Decision Tree|.790|.327|.337|.793|.332|\n",
    "|Random Forest|.846|.479|.229|.863|.310|\n",
    "|SVC|N/A|N/A|N/A|.862|N/A|\n",
    "|K Nearest Neighbor|.866|.66|.269|.871|.382|\n",
    "\n",
    "Due to the drawbacks accuracy can have when evaluating a classifier, I am choosing to focus on Precision and Recall. The classifiers that have the highest precision are:\n",
    "- Naive Bayes\n",
    "- K Nearest Neighbor\n",
    "\n",
    "The classifiers that have the highest recall are :\n",
    "- Decision Tree\n",
    "- Naive Bayes\n",
    "\n",
    "Metrics for SVC was not recorded due to errors. I am going to try some feature scaling and parameter tunes to see if that helps the issue now and re-evaluate. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator found by grid search:\n",
      "SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "{'C': 1000}\n",
      "3\n",
      "0.204395604396\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features_list=['poi','salary',  'bonus', 'total_stock_value', 'exercised_stock_options']\n",
    "data=featureFormat(my_dataset, features_list, sort_keys=True)\n",
    "labels, features=targetFeatureSplit(data)\n",
    "new_finance_features=numpy.array(features) +0.\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "rescaled_features=scaler.fit_transform(new_finance_features)\n",
    "\n",
    "#print \"printing rescaled_features \\n\", rescaled_features\n",
    "\n",
    "param_grid1={'C': [1, 10, 100, 1000],},\n",
    "param_grid2={'C': [1, 10, 100, 1000], 'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1],}\n",
    "param_grid3={'C': [1, 10, 100],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "\n",
    "features_train1, features_test1, labels_train1, labels_test1=\\\n",
    "    train_test_split(rescaled_features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "clf = GridSearchCV(SVC(kernel='rbf'), param_grid1,scoring='recall')\n",
    "clf.fit(features_train1, labels_train1)\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_index_)\n",
    "print(clf.best_score_)    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.897435897436\n",
      "\n",
      "Running 10 fold cross validation to compare accuracy\n",
      "\n",
      "printing mean of 10 fold CV\n",
      "mean =  0.870787545788\n",
      "\n",
      "\n",
      "precision =  0.0\n",
      "recall =  0.0\n",
      "Got a divide by zero when trying out: SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "clf=SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "clf.fit(features_train1,labels_train1)\n",
    "pred=clf.predict(features_test1)\n",
    "#print pred\n",
    "print \"Accuracy is \",clf.score(features_test1, labels_test1)\n",
    "print \"\\nRunning 10 fold cross validation to compare accuracy\\n\"\n",
    "print \"printing mean of 10 fold CV\"\n",
    "print \"mean = \",cross_val_score(clf,rescaled_features,labels,cv=10,scoring='accuracy').mean()\n",
    "print \"\\n\"\n",
    "print \"precision = \", precision_score(labels_test1,pred)\n",
    "print \"recall = \", recall_score(labels_test1,pred)\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "main()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesnt look like feature scaling or parameter tunes helped much in terms of evaulating Precision and Recall. \n",
    "\n",
    "Having good recall means that nearly every time a POI shows up in my test set, I am able to identify him or her. Because of this, I am going to try and tune the classifiers that have the highest baseline of recall to see if I can improve the recall score. I feel this is the important metric to focus on because I want to make sure I dont miss a POI when they show up in the test set. When applying this to the enron case, I feel it is more important to be able to identify POIs even if there are some false positives. False postives can be proven innocent through court trials and rulings made on the individuals. However, if a POI is missed, they could walk free and never be brought to justice. \n",
    "\n",
    "Since parameter tuning is not needed on Naive Bayes, I am going to do paramater tuning on the alogrithms I initially tried. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator found by grid search:\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=3, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='random')\n",
      "{'max_features': 3, 'min_samples_split': 2, 'criterion': 'gini', 'splitter': 'random'}\n",
      "17\n",
      "0.45\n"
     ]
    }
   ],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Tuning Decision Tree\n",
    "\n",
    "\n",
    "param_grid={'criterion':('gini','entropy'),'splitter':('best','random'),'min_samples_split':[2,50,100,1000],\n",
    "                'max_features':[1,2,3,4],}\n",
    "cv = cv = StratifiedShuffleSplit(random_state=42)\n",
    "clf1=tree.DecisionTreeClassifier()\n",
    "clf=GridSearchCV(clf1, param_grid,scoring='recall',cv=cv)\n",
    "clf.fit(features_train, labels_train)\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_index_)\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.897435897436\n",
      "precision =  0.5\n",
      "recall =  0.5\n",
      "\n",
      "Running Stratisfied Shuffle split to compare accuracy\n",
      "\n",
      "printing mean of 10 fold CV\n",
      "mean =  0.25\n",
      "\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=3, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='random')\n",
      "\tAccuracy: 0.78754\tPrecision: 0.31505\tRecall: 0.32450\tF1: 0.31970\tF2: 0.32256\n",
      "\tTotal predictions: 13000\tTrue positives:  649\tFalse positives: 1411\tFalse negatives: 1351\tTrue negatives: 9589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tune Decision Tree\n",
    "clf=tree.DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=3, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "            min_impurity_split=None, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            presort=False, random_state=None, splitter='random')\n",
    "\n",
    "\n",
    "clf.fit(features_train,labels_train)\n",
    "\n",
    "pred=clf.predict(features_test)\n",
    "    #print pred\n",
    "print \"Accuracy is \",clf.score(features_test, labels_test)\n",
    "print \"precision = \", precision_score(labels_test,pred)\n",
    "print \"recall = \", recall_score(labels_test,pred)\n",
    "print \"\\nRunning Stratisfied Shuffle split to compare accuracy\\n\"\n",
    "print \"printing mean of Stratisfied Shuffle Split fold CV\"\n",
    "print \"mean = \",cross_val_score(clf,features,labels,cv=cv.split(features,labels),scoring='recall').mean()\n",
    "print \"\\n\"\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing parameter tuning on Decision Tree using GridsearchCV, i had to set the scorer to recall in order to get the parameters that produced the highest recall. This seemed to help as I now get a recall score of .33. Let see how the other algorithms do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator found by grid search:\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "{'min_samples_split': 2, 'n_estimators': 5}\n",
      "0\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "# Tune Random Forest\n",
    "param_grid={'min_samples_split':[2,50,100],\n",
    "                'n_estimators':[5,10,50,100],}\n",
    "\n",
    "clf1=RandomForestClassifier()\n",
    "clf=GridSearchCV(clf1, param_grid,scoring='recall',cv=cv)\n",
    "clf.fit(features_train, labels_train)\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_index_)\n",
    "print(clf.best_score_)\n",
    "#print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy is  0.820512820513\n",
      "precision =  0.2\n",
      "recall =  0.25\n",
      "\n",
      "Running Stratisfied Shuffle split to compare accuracy\n",
      "\n",
      "printing mean of Stratisfied Shuffle Split fold CV\n",
      "mean =  0.2\n",
      "\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\tAccuracy: 0.82954\tPrecision: 0.41806\tRecall: 0.27550\tF1: 0.33213\tF2: 0.29566\n",
      "\tTotal predictions: 13000\tTrue positives:  551\tFalse positives:  767\tFalse negatives: 1449\tTrue negatives: 10233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\n",
    "\n",
    "clf.fit(features_train,labels_train)\n",
    "\n",
    "pred=clf.predict(features_test)\n",
    "    #print pred\n",
    "print \"Accuracy is \",clf.score(features_test, labels_test)\n",
    "print \"precision = \", precision_score(labels_test,pred)\n",
    "print \"recall = \", recall_score(labels_test,pred)\n",
    "print \"\\nRunning Stratisfied Shuffle split to compare accuracy\\n\"\n",
    "print \"printing mean of Stratisfied Shuffle Split fold CV\"\n",
    "print \"mean = \",cross_val_score(clf,features,labels,cv=cv.split(features,labels),scoring='recall').mean()\n",
    "print \"\\n\"\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing some parameter tuning on RandomForest, my recall improves a little bit but not enough as it is still below .3. I will try parameter tuning on some other algorithms and see if I can get the recall score above .3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator found by grid search:\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform')\n",
      "{'n_neighbors': 1}\n",
      "0\n",
      "0.35\n"
     ]
    }
   ],
   "source": [
    "# Tune K Nearest Neighbors\n",
    "k=numpy.arange(10)+1\n",
    "param_grid={'n_neighbors':k}\n",
    "\n",
    "clf1=KNeighborsClassifier()\n",
    "clf=GridSearchCV(clf1, param_grid,scoring = 'recall',cv=cv)\n",
    "clf.fit(features_train, labels_train)\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_index_)\n",
    "print(clf.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.846153846154\n",
      "precision =  0.333333333333\n",
      "recall =  0.5\n",
      "\n",
      "Running Stratisfied Shuffle split to compare accuracy\n",
      "\n",
      "printing mean of Stratisfied Shuffle Split fold CV\n",
      "mean =  0.3\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform')\n",
      "\tAccuracy: 0.84369\tPrecision: 0.48973\tRecall: 0.38150\tF1: 0.42889\tF2: 0.39914\n",
      "\tTotal predictions: 13000\tTrue positives:  763\tFalse positives:  795\tFalse negatives: 1237\tTrue negatives: 10205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
    "           weights='uniform')\n",
    "\n",
    "\n",
    "clf.fit(features_train,labels_train)\n",
    "\n",
    "pred=clf.predict(features_test)\n",
    "    #print pred\n",
    "print \"Accuracy is \",clf.score(features_test, labels_test)\n",
    "print \"precision = \", precision_score(labels_test,pred)\n",
    "print \"recall = \", recall_score(labels_test,pred)\n",
    "print \"\\nRunning Stratisfied Shuffle split to compare accuracy\\n\"\n",
    "print \"printing mean of Stratisfied Shuffle Split fold CV\"\n",
    "print \"mean = \",cross_val_score(clf,features,labels,cv=cv.split(features,labels),scoring='recall').mean()\n",
    "print \"\\n\"\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing some parameter tunes on my classifier algorithms, I observed the following results:\n",
    "\n",
    "|Classifier|Accuracy|Precision|Recall|Mean 10 Fold CV Accuracy|F1 Score|True positives|False positives|False negatives|True negatives|\n",
    "|-------|------|------|-------|------|------|-------|------|------|-----|\n",
    "|Decision Tree|.796|.33|.311|.823|.319|621|1272|1379|9728|\n",
    "|Random Forest|.833|.433|.284|.854|.342|567|743|1433|10257|\n",
    "|K Nearest Neighbor|.844|.49|.382|.87|.492|763|795|1237|10205|\n",
    "\n",
    "\n",
    "Cleary, the algorithm the benefited the most from parameter tuning was K Nearest Neighbor. It achieved the highest recall from all the classifiers I explored. Now I am going to use K Nearest Neighbor with the optimal value for K and evaluate the precision and recall i get when testing the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.846153846154\n",
      "precision =  0.333333333333\n",
      "recall =  0.5\n",
      "\n",
      "Running Stratisfied Shuffle split to compare recall\n",
      "\n",
      "printing mean of Stratisfied Shuffle Split fold CV\n",
      "mean recall =  0.3\n",
      "mean precision =  0.35\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform')\n",
      "\tAccuracy: 0.84369\tPrecision: 0.48973\tRecall: 0.38150\tF1: 0.42889\tF2: 0.39914\n",
      "\tTotal predictions: 13000\tTrue positives:  763\tFalse positives:  795\tFalse negatives: 1237\tTrue negatives: 10205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run K Nearest Neighbor with optimal K Value and dump out performance metrics\n",
    "\n",
    "clf=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
    "           weights='uniform')\n",
    "\n",
    "\n",
    "clf.fit(features_train,labels_train)\n",
    "\n",
    "pred=clf.predict(features_test)\n",
    "    #print pred\n",
    "print \"Accuracy is \",clf.score(features_test, labels_test)\n",
    "print \"precision = \", precision_score(labels_test,pred)\n",
    "print \"recall = \", recall_score(labels_test,pred)\n",
    "print \"\\nRunning Stratisfied Shuffle split to compare recall\\n\"\n",
    "print \"printing mean of Stratisfied Shuffle Split fold CV\"\n",
    "print \"mean recall = \",cross_val_score(clf,features,labels,cv=cv.split(features,labels),scoring='recall').mean()\n",
    "print \"mean precision = \",cross_val_score(clf,features,labels,cv=cv.split(features,labels),scoring='precision').mean()\n",
    "print \"\\n\"\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from above, when running Stratisfied Shuffle Split, I achieve a mean recall of .3 and a mean precision of .35. When the tester.py file is called, I achieve a recall of .382 and a precision of .49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load tester.py\n",
    "#!/usr/bin/pickle\n",
    "\n",
    "\"\"\" a basic script for importing student's POI identifier,\n",
    "    and checking the results that they get from it \n",
    " \n",
    "    requires that the algorithm, dataset, and features list\n",
    "    be written to my_classifier.pkl, my_dataset.pkl, and\n",
    "    my_feature_list.pkl, respectively\n",
    "\n",
    "    that process should happen at the end of poi_id.py\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\\n",
    "\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "                print \"All predictions should take value 0 or 1.\"\n",
    "                print \"Evaluating performance for processed predictions:\"\n",
    "                break\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "        print \"Precision or recall may be undefined due to a lack of true positive predicitons.\"\n",
    "\n",
    "CLF_PICKLE_FILENAME = \"my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = \"my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = \"my_feature_list.pkl\"\n",
    "\n",
    "def dump_classifier_and_data(clf, dataset, feature_list):\n",
    "    with open(CLF_PICKLE_FILENAME, \"w\") as clf_outfile:\n",
    "        pickle.dump(clf, clf_outfile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"w\") as dataset_outfile:\n",
    "        pickle.dump(dataset, dataset_outfile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"w\") as featurelist_outfile:\n",
    "        pickle.dump(feature_list, featurelist_outfile)\n",
    "\n",
    "def load_classifier_and_data():\n",
    "    with open(CLF_PICKLE_FILENAME, \"r\") as clf_infile:\n",
    "        clf = pickle.load(clf_infile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"r\") as dataset_infile:\n",
    "        dataset = pickle.load(dataset_infile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"r\") as featurelist_infile:\n",
    "        feature_list = pickle.load(featurelist_infile)\n",
    "    return clf, dataset, feature_list\n",
    "\n",
    "def main():\n",
    "    ### load up student's classifier, dataset, and feature_list\n",
    "    clf, dataset, feature_list = load_classifier_and_data()\n",
    "    ### Run testing script\n",
    "    test_classifier(clf, dataset, feature_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
